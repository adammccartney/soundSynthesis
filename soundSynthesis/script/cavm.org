
*Introduction to the subject of electronic music*

* Overview
** Course Structure
The lectures are designed as an introduction to Electronic Music,
Sound Design and Creative Programming. There are 10 dates for lectures
that  allow us to make this brief introduction. Us refers to the
teachers,  students and me the course lecturer as well as the author
of these notes.  Throughout the course, an informal and personal way
of addressing  will be used to explain specific tasks and convey
information.  This will be familiar to those who have attended
lectures at the  Vienna Music Institute as well as something new to
some  who are used to the more formal approach.

One of the key principles we will focus on over the next semester is
the  idea of ​​parametric design. And, in essence, this series of
lectures  can be described in detail as a general introduction to some
basic principles of parametric design. At lecture 5, this topic will
be  presented in detail, it will be enough for us now and in the
upcoming  lectures to get to know the individual modules such as
oscillators,  filters, amplifiers, sequencers and aspects of control. 


** Teaching Philosophy
The subject can be subdivided into the following categories

    + Project Organisation
    + Sound Design
    + Sound Synthesis
    + Digital Signal Processing (DSP)
    + Function and algorithm design
    + Parametric Control

In creating these categories, I’ve tried to represent some of the core
principles of electronic music and also to do this in a way that
allows  some access to the first principles at the root of design and composition. 

** A Symbolic Patch Language for Electronic Music 

To represent particular concepts, we‘ll be using a graphical patch
language  throughout the course. It will help to know for now some of
the basic elements, objects or modules that we will need to
represent. 

#+BEGIN_SRC artist

[osc]        <amp>        |\env/|        *filt*        °°seq°°        

dac|>        |<adc

#+END_SRC


It might help to think about a patch as a kind of instrument, and these modules as the individual parts of that instrument. Surprisingly, it‘s not always immediately obvious to everyone that a guitar is created by setting together a number of parts like: strings, body, neck, fret board, bridge, etc. The manner of construction has been informed by the physical principles of acoustics and also the practical necessities of musicians. In this course it will be of huge advantage to you to understand some basic things about acoustics and your own practical needs as a musician. Of course, we won‘t be making guitars, but as I already mentioned, it helps to think about creating electronic music in terms of putting together instruments and using them to create music. 

** Portability
The aim of the lectures is to learn the basics of electronic music. In this sense, we focus primarily on the principles and techniques that apply to the creation of electronic music, and secondarily on the learning of paradigm-specific methods.
In any case, basic principles can best be illustrated by practical examples. Sometimes it's easier for me to introduce certain concepts in the classroom with proprietary software. In such cases, the goal should not be to spend all your money on acquiring copies of the software you are using, but to find out how the techniques described can be practiced with freely available software. In the first place, you should use two programs in combination to work through the examples that appear in each lesson and work as homework assignments: VCV Rack and Reaper. 

** Reading Material

A very good resource if you want to brush up on some of the basics of the types of mathematics that are found at the root of musical systems
+ Gareth Loy, *Musimathics vol. I-II*, MIT Press

Great reference for many of the essential topics of computer music
+ Curtis Roads: *The Computer Music Tutorial*, MIT Press 

Very well written book on modular synthesis, with lots of worthwhile exercises that will form the basis of our practical work throughout the course
+ Allan Strange: *Electronic Music Systems Techniques and Controls*, Wm-C Brown Company Publishers, 1972


* Some fundamentals of Audio Synthesis
** Oscillators and Wave-forms
Typically an oscillator will be at the root of a synthesis patch. 
In order to hear a specific pitch, one can set the oscillator to 
generate a voltage at the desired frequency.  
Note that there is also a relationship between pitch and perceived 
loudness: a tone played at 1024Hz will sound louder than a tone played with the same gain at 64 Hz.

*** Sine Waves and the Harmonic Series
Come to think of it, this could be a good time to mention the harmonic
series, we'll come back to the relationship between pitch and loudness
in a minute. 

[[./images/Harmonic_Series1.png]]

In the world of synthesizers and especially electronic music, an
oscillator will typically be looking for a precise definition of pitch
in order to produce a tone. 

| Pitch name | partial num. | Frequency (Hz) |
|------------+--------------+----------------|
| c,         |            1 |             64 |
| c          |            2 |            128 |
| g          |            3 |            192 |
| c'         |            4 |            256 |
| e'         |            5 |            320 |
| g'         |            6 |            384 |
| bf'        |            7 |            448 |
| c''        |            8 |            512 |
| d''        |            9 |            576 |
| e''        |           10 |            640 |
| gf''       |           11 |            704 |
| g''        |           12 |            768 |
| af''       |           13 |            832 |
| bf''       |           14 |            896 |
| b''        |           15 |            960 |
| c'''       |           16 |           1024 |
#+TBLFM: $3=$2*64


+ base frequency = f_b
+ partial number = p_num

To calculate partial frequencies along the harmonic series:
*f_x = f_b * p_num*

+ eg: *f(11) = 11 * 64 = 704*

** A side note on midi to Hertz conversion
Practically speaking, you will rarely have to think about doing these
types of conversions from a specific pitch to a midi number. Midi is a
really useful protocol that maps an equal tempered tuning system to
integer values in the range of (0-127).

* Approaches to synthesis

Here is a link to a very interesting website that depicts how the
basics of additive synthesis work. 

https://teropa.info/harmonics-explorer/

** Waveform Types
Of course, a spectrum analysis of the tone produced by a musical
instrument would reveal the presence of individual frequency
components. These are basically packets of energy focused around
certain points in time. Pythagoras or Plato or one of those fellows
would have probably started waxing lyrical about the harmony of the
spheres at this point. 
The main point here is that you don't need to think too deeply about
the concept of /spectrum analysis/ for now, maybe think about it as a
type of sonic x-ray that can reveal some interesting truths about the
nature of a sound. 
The whole point of mentioning analysis at all is that it can be quite
useful when used in combination with synthesis. If fact the practice
of the /analytic-synthetic/ method also goes all the way back to the
Greeks. But that's getting slightly off-topic.   

To summarize about wave-forms: depending on how they are combined,
simple sine-wave components can create more complex wave types. The
tones produced by musical instruments sound themselves complex and can
be analyzed to reveal the underlying structure. There are a few main
types of waveform that are typically found as settings on an oscillator.

[[./images/waveforms.png]]

** Oscillator Controls 
*** Tuning
+ Offset
+ Fine-tune

Obviously, one of the first steps for most endeavours that involve
music, one of the first steps is to tune your instruments. Think about
a band or an orchestra that are going to play a specific melody
together, the instruments and voices need to all agree upon a specific
reference pitch and tuning system to play in before they start
thrashing out melodies and harmonies. 

This example of Ravel's Bolero is sometimes referred to as an example
of an "additive" musical structure. The way that he chooses to
orchestrate the melody is by calling for a number of different
instruments to "double" at different harmonic (as in series)
intervals. 

#+ATTR_HTML :width 50
[[file:./images/ravel_bolero.png]]


*** Pitch input
+ Midi
+ Control Voltage

*** Frequency Modulation (FM)
We'll go into some more detail about frequency modulation (and
modulation in general) later on in the course. It just might be
useful to point out that most oscillators will typically have a
control for FM.

*** Pulse width
On the square wave setting, it is possible to control the length of
the /duty cycle/ of a square wave. Basically, if we were using the
oscillator to open some sort of a gate, the gate would remain open for
the length of time that the square wave is non-zero. 

#+BEGIN_SRC artist
      _____       _____       _____
     |     |     |     |     |     |
     |     |     |     |     |     |
_____|     |_____|     |_____|     |_____
      ___         ___         ___
     |   |       |   |       |   |
     |   |       |   |       |   |
_____|   |_______|   |_______|   |________
      _           _           _
     | |         | |         | |
     | |         | |         | |
_____| |_________| |_________| |__________

#+END_SRC

** Additive Synthesis
Very simply put, additive synthesis is the idea of building up complex
sounds from very, very simply components. Most typically these are sin
tones. The link above connects to an interesting application written
in JavaScript. It's possible to see how the combination of different
sets of partials can produce specific wave shapes. 

*** Building a Hammond organ
This series of videos describes how to build a Hammond organ style
synthesizer using vcv rack: https://youtu.be/kZJF50joo2w


** Subtractive Synthesis

****  Envelopes
In the patch below, we make reference to an "ADSR" envelope for the
first time. This refers to a transient generator that has four
components: Attack, Decay, Sustain and Release. Of these four, three
control temporal aspects of the transient waveform that is produced
(ADR) and the remaining control for sustain controls the amplitude of
the waveform after the decay from a peak amplitude is complete. 

*** Basic Patch Structur
[[./basicSubtractive.png]]

* Filtering techniques
There are many interesting and musical ways to use filters in the
context of modular synthesis. This section briefly outlines some of
the initial approaches that might be interesting. It is highly
encouraged to do your own experiments with filters. Some basic initial
approaches are outlined below and demonstrated in the accompanying
patches. 

** Note on amplifiers and filters
It's worth pointing out at this point that there is a inverse proportianal
relationship between loudness and pitch. Basically, as the pitch
increases, a decrease in gain is necessary in order to maintain the
same dynamic. The opposite is also true, when lowering the pitch, it
is important to increase the gain in order to maintain the same
dynamic level. 

Another thing that's worth mentioning is that it's useful to think of
a filter as basically a very specific type of amplifier i.e. one that
selectively amplifies components of a signal depending on the settings
that are "programmed" in by the user / sound designer or audio
developer. The specific "type" of filter (LP/HP/BP...) determines to a
large degree what specific functionality is available. 

** Low Pass Filters
Everything above the *cutoff* frequency is filtered out. For most
musical uses, the filter cutoff will be controlled 

*** Control of Cutoff via Envelope
This is the case that we documented in the patch /classicSutractive/
where a single ADSR envelope is used to control the amplitude and
frequency of the sound. 

*** Control of Cutoff via Offset and Track
In this case, the filter cutoff is tuned to a particular offset
value (e.g. 8ve + 5th above root) this ensures that the resultant
sound has a stable spectrum because it always contains the same
frequency components. 

In the example /trackingSubtractive/ the VCO is tuned to  261.63 Hz
while the cutoff of the filter is offset to a value 1.5 times higher
than this (261.63 * 1.5 = 392.445)

In the patch /lp_integralNoise/ two banks of filters are used. They are
tuned to a natural harmonic seventh chord (1, 2, 5, 7) partials. The
resonance on the first bank is turned up to full so that the filters
go into self oscillation. The second bank receives audio input from a
white noise source, in order to "fill out" any partials that lie below
the control signal that is used to change the cutoff frequency.

** High Pass Filters
As you might expect, high pass filters pass all frequencies that exist
above a certain cutoff. When applied to a single waveform coming from
an oscillator source, the sound is the quite familiar sweep up the
overtone series. 

However, when used on multiple simultaneous waveforms, hp filters can
be a useful way to create new and interesting tone colors that would
otherwise be difficult to achieve. 

The patch /hp_colorExploration/ contains two oscillators that are
producing triangle waves tuned to a perfect major third in a low
register. When we apply a high-pass filter to eliminate the lower
harmonics, we are left with some uncanny tonal textures. The filter
cutoff can be changed either manually or automated by using the output
of a low frequency oscillator (LFO).  

** Band Pass Filters
Another commonly occuring type of filter is the band pass filter, this
is basically a combination of lp and hp, so everything but a narrow
"band" of frequencies in the middle of the spectrum is filtered
out. There are some really interesting ways to use this technique. 

The patch /bp_colorExploration/ iterates on the previous patch, the
oscialltors are tuned to a more "stable" perfect fifth and the output
of the high pass routed into a second low pass filter. This means that
many of the high end noise that was occurring in the previous examples
can be filtered out and for a more predictable "harmonic" result.




   
* Musical Signals
** Simple Control of Pitch using LFOs
LFO = low frequency oscillator

Range of audible frequencies 15 - 20.000Hz -> stuff that can be heard
as a pitch!

That means anything below 15Hz doesn't sound

*** The value of Hertz

1 Hertz = 1 cycle of a waveform per second
1 cycle per second = 60 cycles per minute
Tempo of a 1 Hz LFO = 60 BPM clock

*** Pitch sweep
This possibly the most simple type of functionality and can be achieved
pretty easily. Simply connect the output of an LFO to the pitch input
of an oscillator that is tuned to sound at audio rate. There are a few
things to note:
+ If you want to have control over the range of pitch that gets
  sweeped, you need to include an attenuator in the signal path
+ The pitches output are unquantized (they don't conform to any
  particular scale)

*** Sample & Hold
The first step towards getting the sort of discrete steps that we
would need to make up a musical scale is isolating individual pitches
within the context of our filter sweep, and ensuring that the control
signal ("CV") stays at one specific pitch long enough for us to
perceive it as a note. See the patch "sampleHold1.vcv" for details on
how to patch this up.
In this patch we are introducing the "Random" module, an interesting
base module in the VCV suite. It is programmed to generate random
values along one of two possible distriubutions. These random voltages
are sampled according to the rate function, and they can be further
divided up into a maximum of 16 steps using the "shape" slider. It's a
useful module and I highly recommend reading the short documentation
to get a better understanding of what it does. 

Some notable features of this pitch. 
+ At this stage, we might seem to be approaching something that could
  possibly be described as a musical signal.
+ With some clever routing of our modules, we can also control other
  aspects of the sound


** Sequencers
*** Modular Style
The most typical style of sequencer found in modular systems are those
with rows of pots (basically potentiometers) that are used to regulte
the outgoing cv. There is either an internal clock that powers the
steps, or the sequencer is fed in a clock pulse from an external
source. 
A pot is used to set the specific voltage at a given step, and then
this gets combined with a gate signal that is typically used to
trigger some sort of an envelope. This means that the modular style
sequencer's output is usually routed to two sources. 

*** DAW Style
The "piano roll" is one of the most common ways to sequence in a daw,
basically this is a graphic user interface that maps an image of a
piano keyboard on to the screen and the user basically drops in notes
where they would like them to sound. Sequences can typically be of
variable length.


** Quantization
*** Pitch Quantization

Oscillators are outputting an audio signal, the pitch control is
enabling control over what frequencies are being put out. For all
intents and purposes this control enables continuous access to all
pitches along the frequency spectrum. If we don't impose any rounding
on these values as they come out of the oscillator, we refer to the
audio signal as unquantized. 

Although obviously it can be applied to pitch very effectively,
quantization steps can be applied to "CV" signals also. This means
that you can basically run a control signal through a quantizer in
order to "whip it into shape" before passing it to the CV input of an
oscillator. 

*** Offsets & Harmonization
While we're on the topic of quantization, another cool thing that you
can do with quantizers is to use them in combination with some
offsets. 
Basically, you can take a single cv signal and pass it through an
offset/attenuator in order to change the range and amplitude of the
signal. If you pass the offset signal back into a quantizer tuned to
the same pitches as your second signal, you get instant access to
diatonic or tonal harmonization. 

One further interesting thing to try out is to further mix the offset
signal with another signal coming from an LFO. This quickly gives rise
to many variations on the original seuqence, as it is harder to
predict exactly how the sequences are related to one another.

*** Clock Subdivision
In order to have a bit more control over what's coming out of the
quantizer, it's often useful to use a clock multiplier/divider to
create a rhythmical relation to the amplitude envelope of your second
signal. 


* Troubleshooting

** Patch description

+ We're sending noise to a filter
+ Want to control the filtercutoff with a keyboard
+ We want a pitched noise output

* Modulation
** Sub-Audio Rate Modulation
*** Definition
When we're taking about modulation, typically we differentiate between
two different types of signal that are involved:

+ the *carrier* is basically considered the basis for the modulation,
  the tone that gets manipulated or that "carries" the modulator
+ the *program or modulator* is the signal that is causing some sort of
  periodic fluctuations to the carrier

If you're using a Low Frequence Oscillator to some how "animate" the
signal on it's way to becoming audio, you can pretty much consider
this sub-audio rate modulation. As we've mentioned before, anything
that occurs below 15hz is below the threshhold for what humans can
perceive as pitch and so instead we typically here it as rhythm. 

*** Amplitude Modulation (AM)
If we're using an lfo to do amplitude modulation, then we are
typically mixing together the signal coming from an LFO with the
transient that is creating our envelope. This produces a kind of
"amplitude vibrato" where the volume of the tone seems to flutter. 

*** Frequency Modulation (FM)
If the carrier is applied to the FM input of the modulator, then the
audible effects resemble that of a tremolo between two distinct
pitches.

** Audio Rate Modulation
*** Side bands and Timbre
*** FM
*** AM 



* Sound Synthesis
  
The following is a cherry-pick and very brief exposition of a couple
of topics related to an area of sound synthesis that I've found
personally interesting over the years. These topics are all covered in
excellent detail in Gareth Loy's book Musimathics vol. II. 

** Delay Lines
Following on from the books disucssion, here are two ways to implement
a delay line. Although we haven't been worrying about the details of
implementation throughout this course, it's not a bad idea to
occasionally point to one or two as if we are serious scholars. As an
aside, I once submitted PhD application to Trinity College Dublin with
the glorious notion of writing my dissertation on "something about
time". If nothing else, this brief discussion of exactly what a delay
is might serve as a reminder that all of this music stuff seems to
have something or other to do with time.

*** Mexican Wave
(The actual example in the book is Bucket Brigate, but I've always
been a big personal fan of the mexican wave). Imagine that the year is
1986. The FIFA world cup is on and you have tickets to the final where
Argentina play against West Germany. At different points during the
match, the crown begins "passing a wave" around the stadium. The idea
is that the peak of the wave is passed by one section of the crowd
standing up and then returning to their seats as the peak moves to the
next acjacent section.
[[https://en.wikipedia.org/wiki/File:AUSvIRE4Wave.ogv]]

For the Bucket Brigade or Mexican Wave analogy to work, you have to
put yourself in the position of imagining that you are the peak of the
wave travelling around the stadium, you are essentially being passed
from one section of the crowd to the next.

To bring the anology a bit closer to the computer, imagine the
sections of the crowd as an array of samples, each time the "wave"
passes along the array, a sample is moved from one index to the next,
or from the input stack to the output stack along a FIFO (first in
first out) queue. This is not to be confused with FIFA of course,
which stands for the Fédération Internationale de Football Association.

*** In-place method
The inplace method is the same as above, there is a queue and an input
and output stack, but this time for the analogy you have to imagine
yourself as an individual in the crowd. You experience the peak of the
wave when it passes, and then you have to sit and wait again for n
seconds for the wave to come around a second time.

*** Echo modelling
Imagine a signal path: 
#+begin_src artist

(sound source) -> [ADC] -> [---delay---] -> [DAC] -> |) 

#+end_src

If you place a delay module in the signal path and that delays the 
sound by /N/ samples. The sound will arrive at the output /N/ seconds
after the sound has left the input.

*** Screaming into a cave
At one time or another, we've all screamed into a cave. Not the empty
cave of our dreams, but a real physical cave with damp, stalactites
and formerly bears. You may recall that the echo that you hear
dissipates on it's return. Well, this dissipation is something that we
can try to model with our syntehsis system in order to recreate an
accurate sounding audible delay. 

In real world acoustical situations (i.e. a room full of air) the
pressure of a transverse, spherical waveform drops off at 1/d. That is
to say the amplitude decreases as the delay time increases. 
  
If the delay line is /N/ samples and /T/ seconds long, it's possible
to model the attenuation of a pressure wave by setting a = 1/(/NT/)
Intesity (perceived loudness) drops off as d^2, so we can write 
a = 1/(/NT/)^2

*** Mercilessly skipping ahead to some sound examples

Okay, so real physical situations are a bit more complicated that this
simple model. Typically if there is a sound source playing in a room,
the room will give back many more reflections than the sound
source. This is also true for the bodies of instruments, where tiny
reflections can give a specific violin, cello, guitar, drum, marimba,
its own very distinctive quality. There is a whole area of synthesis
research known as physical modelling, where the various types of delay
networks are compiled to create many interesting sounds. 
 
** Karplus strong
Physical modelling synthesis sort of begins though in a very simple
way, with a type of algorithm known as karplus strong. It's very
simple infact, all you need is a noise source, an amplifier, a vca and
an output. By modulating aspects of the delay time and feedback, you
can recreate the sound of a pitched string.

I've created a patch called "karplus-really-Strong.vcv" to demonstrate
some of the tecniques mentioned above.

** Reverb (delay in context)

Again, the following sections are largely referring to chapter 9 of
Musimathics vol. 2 by Gareth Loy, a really excellent resource on this
topic. 

Now that we've had a chance to look at delay lines and what they are
all about, let's try to put them in context. Delay lines can be nicely
framed within the context of hall acoustics or reverb. Essentially,
simple delay lines can be thought about as the bricks and mortar of
acoustic spaces (as well as physical models of instruments as we have
seen with the karplus strong algorithm).

*** Comb filtering (multipath wave propagation)
Okay, so we're going to do use a couple of the very simple equations
that are presented in the book to try to model some different types of
room and instrument setup. For that we're going to use the constant
value of the speed of sound in air which is 343m/s.

#+Begin_src 

Distance from flute to microphone is sqrt(2)m
distance from flute to floor and on to microphone 2m
 
Distance of direct path: sqrt(2) ~= 1.414m
Distance of reflection: (2 - 1.414)m

Duration of the delay: 0.586/343 = 1,708 ms

#+end_src

The sound arriving at the microphone or eardrum will be the result of
constructive / destructive interference based on the interaction of
the sounds sinusoidal components. 

This shows us something important about modelling room acoustics: that
the ratio between the delay lines is important and what ultimately
determines the color of a specific room.

*** Multitap delay 

The next step is to simply use this model to work with a multitap
delay. A multitap delay is simply analogous to the sound in most
natural settings where there are a number of reflective surfaces that
result in myriad reflections of the sound and ultimately a sort of
syntehsis occuring at the point of reception / perception (I can't
think how to best say this ... when it hits your earholes!).


primary source: 3m, delay=8.7ms

| reflection | distance | delay (ms) |
|------------+----------+------------|
|          1 |        5 |       14.5 |
|          2 |        8 |       23.3 |
|          3 |       13 |       37.9 |
|          4 |       21 |       61.2 |


* Project work
** in a DAW
** Voice Assignment
   
** Appendix A
*** Program selection, installation and directory structure
*** Project organisation in Digital Audio Workstations







